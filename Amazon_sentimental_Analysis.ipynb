{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sristiahuja/Marketing-research-methods---SristiAhuja-_-Jio-Institute/blob/main/Amazon_sentimental_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPiVaNJIHGsI",
        "outputId": "cb699462-cce3-4cab-d099-a8a3881b8e6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.155.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "pip install google-api-python-client textblob pandas nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Download NLTK stopwords (for analyzing concerns)\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define API credentials\n",
        "API_KEY = 'AIzaSyBfi3HVFk2J4CVfZqAbJaf118qJ0QCWeoM'  # Your API key\n",
        "VIDEO_ID = 'rj6JOKrL_vg'  # Your video ID\n",
        "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
        "YOUTUBE_API_VERSION = 'v3'\n",
        "\n",
        "# Function to fetch YouTube video comments\n",
        "def fetch_comments(video_id, max_results=100):\n",
        "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
        "    comments = []\n",
        "    try:\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=max_results,\n",
        "            textFormat='plainText'\n",
        "        ).execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching comments: {e}\")\n",
        "\n",
        "    return comments\n",
        "\n",
        "# Function to analyze sentiments\n",
        "def analyze_sentiments(comments):\n",
        "    sentiment_data = []\n",
        "    for comment in comments:\n",
        "        blob = TextBlob(comment)\n",
        "        sentiment_score = blob.sentiment.polarity  # Polarity: -1 (negative) to +1 (positive)\n",
        "        sentiment = 'Positive' if sentiment_score > 0 else 'Negative' if sentiment_score < 0 else 'Neutral'\n",
        "        sentiment_data.append({\n",
        "            'Comment': comment,\n",
        "            'Polarity': sentiment_score,\n",
        "            'Sentiment': sentiment\n",
        "        })\n",
        "    return pd.DataFrame(sentiment_data)\n",
        "\n",
        "# Function to identify key concerns\n",
        "def extract_concerns(comments):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    keywords = []\n",
        "\n",
        "    for comment in comments:\n",
        "        words = comment.lower().split()\n",
        "        keywords.extend([word for word in words if word not in stop_words and len(word) > 3])\n",
        "\n",
        "    # Find the most common keywords\n",
        "    keyword_counts = Counter(keywords)\n",
        "    return keyword_counts.most_common(10)\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == '__main__':\n",
        "    print(\"Fetching comments...\")\n",
        "    comments = fetch_comments(VIDEO_ID, max_results=50)\n",
        "\n",
        "    if comments:\n",
        "        print(f\"Fetched {len(comments)} comments. Analyzing sentiments...\")\n",
        "        sentiment_df = analyze_sentiments(comments)\n",
        "        print(sentiment_df)\n",
        "\n",
        "        # Save sentiment results to a CSV\n",
        "        sentiment_df.to_csv('youtube_comments_sentiment_analysis.csv', index=False)\n",
        "        print(\"Sentiment analysis completed and saved to 'youtube_comments_sentiment_analysis.csv'.\")\n",
        "\n",
        "        print(\"\\nExtracting customer concerns...\")\n",
        "        concerns = extract_concerns(comments)\n",
        "        print(\"Top customer concerns:\", concerns)\n",
        "\n",
        "        # Save concerns to a CSV\n",
        "        concerns_df = pd.DataFrame(concerns, columns=['Keyword', 'Frequency'])\n",
        "        concerns_df.to_csv('customer_concerns.csv', index=False)\n",
        "        print(\"Customer concerns saved to 'customer_concerns.csv'.\")\n",
        "    else:\n",
        "        print(\"No comments fetched.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Opx8TyfaHPMF",
        "outputId": "89cbfa37-06a6-418a-9b12-c3de3599af60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching comments...\n",
            "Fetched 50 comments. Analyzing sentiments...\n",
            "                                              Comment      Polarity Sentiment\n",
            "0   💰 Do you watch YouTube? if you do, here's how ...  0.000000e+00   Neutral\n",
            "1                           💛💜💛 THANK YOU, TRUTH! ! !  0.000000e+00   Neutral\n",
            "2   If there is a God, may She damn Nestle(vil) to...  0.000000e+00   Neutral\n",
            "3   Companies colonized. Companies colonizing for ...  0.000000e+00   Neutral\n",
            "4                                                   👍  0.000000e+00   Neutral\n",
            "5   looks, it's very simple.\\nas long as these cor...  1.734723e-18  Positive\n",
            "6                  Does the Tran stand for something?  0.000000e+00   Neutral\n",
            "7   I feel bad for Henry his reputation was ruined... -6.333333e-01  Negative\n",
            "8   Do one on sugar and the sugar barons, if you h...  0.000000e+00   Neutral\n",
            "9                      Average Jewish run corporation -7.500000e-02  Negative\n",
            "10  this is real world tof tof and Shit place for ...  0.000000e+00   Neutral\n",
            "11  1 Timothy 6:10 “For the LOVE of money is a roo...  0.000000e+00   Neutral\n",
            "12                                      10 \"x\" my God  0.000000e+00   Neutral\n",
            "13  To bad this generation is not experiencing the... -6.666667e-02  Negative\n",
            "14  They're also in the dairy industry, which is t... -3.666667e-01  Negative\n",
            "15          Nestle chocolate taste......ugh. ..  ...!  0.000000e+00   Neutral\n",
            "16  Big companies are bigger than you think. Amazo... -3.333333e-02  Negative\n",
            "17  For me it isn't just nestle . it's the health ...  3.750000e-01  Positive\n",
            "18  Are you Vietnamese? Tran is the Trần family na...  0.000000e+00   Neutral\n",
            "19              Jaká válka to židé se mlátí za peníze  0.000000e+00   Neutral\n",
            "20  Nestle are now hounding NZ agriculture,obvious...  1.000000e-01  Positive\n",
            "21  \"Watet isn't a human right\"\\n\"We own slaves in...  2.309524e-01  Positive\n",
            "22  Everyone say “fair trade”!!!! This is how you ...  5.666667e-01  Positive\n",
            "23  Studies endorse the fact that dairy products c...  5.000000e-02  Positive\n",
            "24                    You don't hate Nestlé enough... -4.000000e-01  Negative\n",
            "25  Wait - this isn't about Nestle Nesquik chocola...  0.000000e+00   Neutral\n",
            "26  Until stronger action is taken by the common m... -1.625000e-01  Negative\n",
            "27  1st Timothy 6:10\\nFor the love of money is the...  1.250000e-01  Positive\n",
            "28  2:38 making a poster illustrating a pregnant w... -3.333333e-01  Negative\n",
            "29  Nestle has infiltrated the supplement business...  3.000000e-01  Positive\n",
            "30  I can tell a few stories of Nestle being a bit...  7.790179e-02  Positive\n",
            "31                Nestlé should be banned altogether.  0.000000e+00   Neutral\n",
            "32  8:24 i am literally about to punch my laptop s...  0.000000e+00   Neutral\n",
            "33  I'm gonna take a break from this Nestlé stuff,...  0.000000e+00   Neutral\n",
            "34        nestlé.\\nsincerely.\\nyou ohio sigma skibidi  5.000000e-01  Positive\n",
            "35                                                Smh  0.000000e+00   Neutral\n",
            "36                   the classic get rich Quik scheme  2.708333e-01  Positive\n",
            "37  I think the joke here is thinking these big co...  0.000000e+00   Neutral\n",
            "38  I thought friendly and civilised people live i...  2.556818e-01  Positive\n",
            "39           Which language  nestle chairman talking?  0.000000e+00   Neutral\n",
            "40                          Nestle is an evil company -1.000000e+00  Negative\n",
            "41  Humans are evil, i need to get out of here, ev... -1.000000e+00  Negative\n",
            "42  These things are what we know, who knows what ...  5.000000e-01  Positive\n",
            "43  Its insane how this is still allowed to be a c... -3.625000e-01  Negative\n",
            "44                                           Sources?  0.000000e+00   Neutral\n",
            "45  Ever since I heard about Nestlé's actions, I h...  0.000000e+00   Neutral\n",
            "46  I'm glad Palestine made me aware of how evil N... -8.333333e-02  Negative\n",
            "47  KitKats have been disgusting ever since nestle... -1.000000e+00  Negative\n",
            "48  Самая большая проблема,что nestle не возможно ...  0.000000e+00   Neutral\n",
            "49                  Well done, I watched till the end  0.000000e+00   Neutral\n",
            "Sentiment analysis completed and saved to 'youtube_comments_sentiment_analysis.csv'.\n",
            "\n",
            "Extracting customer concerns...\n",
            "Top customer concerns: [('nestle', 12), ('companies', 5), ('people', 4), ('evil', 4), ('take', 4), ('supposed', 4), ('since', 4), ('even', 4), ('ever', 3), ('local', 3)]\n",
            "Customer concerns saved to 'customer_concerns.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords (for analyzing concerns)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define API credentials (replace with your actual API key and video ID)\n",
        "API_KEY = 'YOUR_API_KEY'\n",
        "VIDEO_ID = 'YOUR_VIDEO_ID'\n",
        "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
        "YOUTUBE_API_VERSION = 'v3'\n",
        "\n",
        "# ... (rest of the code remains the same)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WisvMnrWHcjv",
        "outputId": "9024619f-84ee-4e8a-b78e-61f68aedceb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def analyze_nestle_concerns(csv_filepath):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_filepath)\n",
        "        keywords = df['Keyword'].tolist()\n",
        "\n",
        "        # Basic analysis (expand for more insights)\n",
        "        print(\"Top keywords related to negative sentiment towards Nestle:\")\n",
        "        for keyword in keywords:\n",
        "          print(keyword)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{csv_filepath}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage (replace with your actual CSV file)\n",
        "analyze_nestle_concerns('customer_concerns.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTEU6rAlIFYn",
        "outputId": "180a339e-0cb2-41b4-9e1c-233fc5f8c943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top keywords related to negative sentiment towards Nestle:\n",
            "nestle\n",
            "companies\n",
            "people\n",
            "evil\n",
            "take\n",
            "supposed\n",
            "since\n",
            "even\n",
            "ever\n",
            "local\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Install necessary libraries\n",
        "!pip install google-api-python-client textblob pandas nltk\n",
        "\n",
        "# Download NLTK stopwords (for analyzing concerns)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define API credentials\n",
        "API_KEY = 'AIzaSyBfi3HVFk2J4CVfZqAbJaf118qJ0QCWeoM'  # Your API key\n",
        "VIDEO_ID = 'rj6JOKrL_vg'  # Your video ID\n",
        "YOUTUBE_API_SERVICE_NAME = 'youtube'\n",
        "YOUTUBE_API_VERSION = 'v3'\n",
        "\n",
        "# Function to fetch YouTube video comments\n",
        "def fetch_comments(video_id, max_results=100):\n",
        "    youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION, developerKey=API_KEY)\n",
        "    comments = []\n",
        "    try:\n",
        "        response = youtube.commentThreads().list(\n",
        "            part='snippet',\n",
        "            videoId=video_id,\n",
        "            maxResults=max_results,\n",
        "            textFormat='plainText'\n",
        "        ).execute()\n",
        "\n",
        "        for item in response['items']:\n",
        "            comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "            comments.append(comment)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching comments: {e}\")\n",
        "\n",
        "    return comments\n",
        "\n",
        "# Function to analyze sentiments\n",
        "def analyze_sentiments(comments):\n",
        "    sentiment_data = []\n",
        "    for comment in comments:\n",
        "        blob = TextBlob(comment)\n",
        "        sentiment_score = blob.sentiment.polarity  # Polarity: -1 (negative) to +1 (positive)\n",
        "        sentiment = 'Positive' if sentiment_score > 0 else 'Negative' if sentiment_score < 0 else 'Neutral'\n",
        "        sentiment_data.append({\n",
        "            'Comment': comment,\n",
        "            'Polarity': sentiment_score,\n",
        "            'Sentiment': sentiment\n",
        "        })\n",
        "    return pd.DataFrame(sentiment_data)\n",
        "\n",
        "# Function to identify key concerns\n",
        "def extract_concerns(comments):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    keywords = []\n",
        "\n",
        "    for comment in comments:\n",
        "        words = comment.lower().split()\n",
        "        keywords.extend([word for word in words if word not in stop_words and len(word) > 3])\n",
        "\n",
        "    # Find the most common keywords\n",
        "    keyword_counts = Counter(keywords)\n",
        "    return keyword_counts.most_common(10)\n",
        "\n",
        "# Main Execution\n",
        "if __name__ == '__main__':\n",
        "    print(\"Fetching comments...\")\n",
        "    comments = fetch_comments(VIDEO_ID, max_results=50)\n",
        "\n",
        "    if comments:\n",
        "        print(f\"Fetched {len(comments)} comments. Analyzing sentiments...\")\n",
        "        sentiment_df = analyze_sentiments(comments)\n",
        "        print(sentiment_df)\n",
        "\n",
        "        # Save sentiment results to a CSV\n",
        "        sentiment_df.to_csv('youtube_comments_sentiment_analysis.csv', index=False)\n",
        "        print(\"Sentiment analysis completed and saved to 'youtube_comments_sentiment_analysis.csv'.\")\n",
        "\n",
        "        print(\"\\nExtracting customer concerns...\")\n",
        "        concerns = extract_concerns(comments)\n",
        "        print(\"Top customer concerns:\", concerns)\n",
        "\n",
        "        # Save concerns to a CSV\n",
        "        concerns_df = pd.DataFrame(concerns, columns=['Keyword', 'Frequency'])\n",
        "        concerns_df.to_csv('customer_concerns.csv', index=False)\n",
        "        print(\"Customer concerns saved to 'customer_concerns.csv'.\")\n",
        "    else:\n",
        "        print(\"No comments fetched.\")\n",
        "\n",
        "\n",
        "def analyze_nestle_concerns(csv_filepath):\n",
        "    try:\n",
        "        df = pd.read_csv(csv_filepath)\n",
        "        keywords = df['Keyword'].tolist()\n",
        "\n",
        "        # Basic analysis (expand for more insights)\n",
        "        print(\"Top keywords related to negative sentiment towards Nestle:\")\n",
        "        for keyword in keywords:\n",
        "          print(keyword)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{csv_filepath}' not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "\n",
        "# Example usage\n",
        "analyze_nestle_concerns('customer_concerns.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZueegUzvIPEL",
        "outputId": "65436f85-e640-4b8d-8381-062c25b62281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (2.155.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.27.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (2.19.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.25.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.12.14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching comments...\n",
            "Fetched 50 comments. Analyzing sentiments...\n",
            "                                              Comment      Polarity Sentiment\n",
            "0   💰 Do you watch YouTube? if you do, here's how ...  0.000000e+00   Neutral\n",
            "1                           💛💜💛 THANK YOU, TRUTH! ! !  0.000000e+00   Neutral\n",
            "2   If there is a God, may She damn Nestle(vil) to...  0.000000e+00   Neutral\n",
            "3   Companies colonized. Companies colonizing for ...  0.000000e+00   Neutral\n",
            "4                                                   👍  0.000000e+00   Neutral\n",
            "5   looks, it's very simple.\\nas long as these cor...  1.734723e-18  Positive\n",
            "6                  Does the Tran stand for something?  0.000000e+00   Neutral\n",
            "7   I feel bad for Henry his reputation was ruined... -6.333333e-01  Negative\n",
            "8   Do one on sugar and the sugar barons, if you h...  0.000000e+00   Neutral\n",
            "9                      Average Jewish run corporation -7.500000e-02  Negative\n",
            "10  this is real world tof tof and Shit place for ...  0.000000e+00   Neutral\n",
            "11  1 Timothy 6:10 “For the LOVE of money is a roo...  0.000000e+00   Neutral\n",
            "12                                      10 \"x\" my God  0.000000e+00   Neutral\n",
            "13  To bad this generation is not experiencing the... -6.666667e-02  Negative\n",
            "14  They're also in the dairy industry, which is t... -3.666667e-01  Negative\n",
            "15          Nestle chocolate taste......ugh. ..  ...!  0.000000e+00   Neutral\n",
            "16  Big companies are bigger than you think. Amazo... -3.333333e-02  Negative\n",
            "17  For me it isn't just nestle . it's the health ...  3.750000e-01  Positive\n",
            "18  Are you Vietnamese? Tran is the Trần family na...  0.000000e+00   Neutral\n",
            "19              Jaká válka to židé se mlátí za peníze  0.000000e+00   Neutral\n",
            "20  Nestle are now hounding NZ agriculture,obvious...  1.000000e-01  Positive\n",
            "21  \"Watet isn't a human right\"\\n\"We own slaves in...  2.309524e-01  Positive\n",
            "22  Everyone say “fair trade”!!!! This is how you ...  5.666667e-01  Positive\n",
            "23  Studies endorse the fact that dairy products c...  5.000000e-02  Positive\n",
            "24                    You don't hate Nestlé enough... -4.000000e-01  Negative\n",
            "25  Wait - this isn't about Nestle Nesquik chocola...  0.000000e+00   Neutral\n",
            "26  Until stronger action is taken by the common m... -1.625000e-01  Negative\n",
            "27  1st Timothy 6:10\\nFor the love of money is the...  1.250000e-01  Positive\n",
            "28  2:38 making a poster illustrating a pregnant w... -3.333333e-01  Negative\n",
            "29  Nestle has infiltrated the supplement business...  3.000000e-01  Positive\n",
            "30  I can tell a few stories of Nestle being a bit...  7.790179e-02  Positive\n",
            "31                Nestlé should be banned altogether.  0.000000e+00   Neutral\n",
            "32  8:24 i am literally about to punch my laptop s...  0.000000e+00   Neutral\n",
            "33  I'm gonna take a break from this Nestlé stuff,...  0.000000e+00   Neutral\n",
            "34        nestlé.\\nsincerely.\\nyou ohio sigma skibidi  5.000000e-01  Positive\n",
            "35                                                Smh  0.000000e+00   Neutral\n",
            "36                   the classic get rich Quik scheme  2.708333e-01  Positive\n",
            "37  I think the joke here is thinking these big co...  0.000000e+00   Neutral\n",
            "38  I thought friendly and civilised people live i...  2.556818e-01  Positive\n",
            "39           Which language  nestle chairman talking?  0.000000e+00   Neutral\n",
            "40                          Nestle is an evil company -1.000000e+00  Negative\n",
            "41  Humans are evil, i need to get out of here, ev... -1.000000e+00  Negative\n",
            "42  These things are what we know, who knows what ...  5.000000e-01  Positive\n",
            "43  Its insane how this is still allowed to be a c... -3.625000e-01  Negative\n",
            "44                                           Sources?  0.000000e+00   Neutral\n",
            "45  Ever since I heard about Nestlé's actions, I h...  0.000000e+00   Neutral\n",
            "46  I'm glad Palestine made me aware of how evil N... -8.333333e-02  Negative\n",
            "47  KitKats have been disgusting ever since nestle... -1.000000e+00  Negative\n",
            "48  Самая большая проблема,что nestle не возможно ...  0.000000e+00   Neutral\n",
            "49                  Well done, I watched till the end  0.000000e+00   Neutral\n",
            "Sentiment analysis completed and saved to 'youtube_comments_sentiment_analysis.csv'.\n",
            "\n",
            "Extracting customer concerns...\n",
            "Top customer concerns: [('nestle', 12), ('companies', 5), ('people', 4), ('evil', 4), ('take', 4), ('supposed', 4), ('since', 4), ('even', 4), ('ever', 3), ('local', 3)]\n",
            "Customer concerns saved to 'customer_concerns.csv'.\n",
            "Top keywords related to negative sentiment towards Nestle:\n",
            "nestle\n",
            "companies\n",
            "people\n",
            "evil\n",
            "take\n",
            "supposed\n",
            "since\n",
            "even\n",
            "ever\n",
            "local\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: I dont want keywords and frequency , I want to understand what it signifies by using NLP please help me in that way and give indepth what we need to change\n",
        "\n",
        "from googleapiclient.discovery import build\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer # For improved sentiment analysis\n",
        "from nltk.stem import WordNetLemmatizer # For lemmatization\n",
        "from collections import Counter\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4') # Open Multilingual Wordnet\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# ... (Your API key and video ID)\n",
        "\n",
        "# Improved Sentiment Analysis\n",
        "def analyze_sentiments(comments):\n",
        "    analyzer = SentimentIntensityAnalyzer()\n",
        "    sentiment_data = []\n",
        "    for comment in comments:\n",
        "        scores = analyzer.polarity_scores(comment)\n",
        "        compound_score = scores['compound']  # Compound score: -1 to +1\n",
        "        sentiment = 'Positive' if compound_score >= 0.05 else 'Negative' if compound_score <= -0.05 else 'Neutral'\n",
        "        sentiment_data.append({\n",
        "            'Comment': comment,\n",
        "            'Compound Score': compound_score,\n",
        "            'Sentiment': sentiment\n",
        "        })\n",
        "    return pd.DataFrame(sentiment_data)\n",
        "\n",
        "# Enhanced Concern Extraction with Lemmatization\n",
        "def extract_concerns(comments, sentiment_df):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    keywords = []\n",
        "\n",
        "    # Only consider negative comments\n",
        "    negative_comments = sentiment_df[sentiment_df['Sentiment'] == 'Negative']['Comment'].tolist()\n",
        "\n",
        "    for comment in negative_comments:\n",
        "        words = comment.lower().split()\n",
        "        keywords.extend([lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 3])\n",
        "\n",
        "    keyword_counts = Counter(keywords)\n",
        "    return keyword_counts.most_common(10)  # Return top 10 concerns\n",
        "\n",
        "\n",
        "# ... (rest of your code - fetch_comments remains the same)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (fetch comments)\n",
        "\n",
        "    if comments:\n",
        "        sentiment_df = analyze_sentiments(comments)\n",
        "        # ... (Save sentiment analysis)\n",
        "\n",
        "        print(\"\\nExtracting customer concerns (from negative comments)...\")\n",
        "        concerns = extract_concerns(comments, sentiment_df)  # Pass sentiment dataframe\n",
        "\n",
        "        # Analyze and print concerns\n",
        "        if concerns:\n",
        "          print(\"Top customer concerns:\")\n",
        "          for keyword, count in concerns:\n",
        "              print(f\"- {keyword} (Count: {count})\")\n",
        "              # Further analysis - categorize keywords, etc.\n",
        "        else:\n",
        "            print(\"No significant negative concerns found.\")\n",
        "\n",
        "        # ... (Save concerns to CSV)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7cmFm5cIzfi",
        "outputId": "18788f5c-71db-47c6-e461-fb0411387ce5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting customer concerns (from negative comments)...\n",
            "Top customer concerns:\n",
            "- nestle (Count: 5)\n",
            "- evil (Count: 4)\n",
            "- supposed (Count: 4)\n",
            "- company (Count: 3)\n",
            "- since (Count: 3)\n",
            "- even (Count: 3)\n",
            "- long (Count: 2)\n",
            "- corporation (Count: 2)\n",
            "- kitkats (Count: 2)\n",
            "- state (Count: 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced Concern Extraction with Lemmatization\n",
        "def extract_concerns(comments, sentiment_df):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    keywords = []\n",
        "\n",
        "    # Only consider negative comments\n",
        "    negative_comments = sentiment_df[sentiment_df['Sentiment'] == 'Negative']['Comment'].tolist()\n",
        "\n",
        "    for comment in negative_comments:\n",
        "        words = comment.lower().split()\n",
        "        keywords.extend([lemmatizer.lemmatize(word) for word in words if word not in stop_words and len(word) > 3])\n",
        "\n",
        "    keyword_counts = Counter(keywords)\n",
        "    return keyword_counts.most_common(10)  # Return top 10 concerns\n",
        "\n",
        "\n",
        "# ... (rest of your code - fetch_comments remains the same)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # ... (fetch comments)\n",
        "\n",
        "    if comments:\n",
        "        sentiment_df = analyze_sentiments(comments)\n",
        "        # ... (Save sentiment analysis)\n",
        "\n",
        "        print(\"\\nExtracting customer concerns (from negative comments)...\")\n",
        "        concerns = extract_concerns(comments, sentiment_df)  # Pass sentiment dataframe\n",
        "\n",
        "        # Analyze and print concerns\n",
        "        if concerns:\n",
        "          print(\"Top customer concerns:\")\n",
        "          for keyword, count in concerns:\n",
        "              print(f\"- {keyword} (Count: {count})\")\n",
        "              # Further analysis - categorize keywords, etc.\n",
        "        else:\n",
        "            print(\"No significant negative concerns found.\")\n",
        "\n",
        "        # ... (Save concerns to CSV)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D93PiBRSJLI7",
        "outputId": "8013c646-dbed-48e9-9492-dbffd6a19fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting customer concerns (from negative comments)...\n",
            "Top customer concerns:\n",
            "- nestle (Count: 5)\n",
            "- evil (Count: 4)\n",
            "- supposed (Count: 4)\n",
            "- company (Count: 3)\n",
            "- since (Count: 3)\n",
            "- even (Count: 3)\n",
            "- long (Count: 2)\n",
            "- corporation (Count: 2)\n",
            "- kitkats (Count: 2)\n",
            "- state (Count: 2)\n"
          ]
        }
      ]
    }
  ]
}